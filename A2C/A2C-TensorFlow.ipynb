{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-Critic-TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the `Keras` API of `TensorFlow2.0` to play `Acrobot-v1` game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The acrobot system includes two joints and two links, where the joint between the two links is actuated. Initially, the links are hanging downwards, and the goal is to swing the end of the lower link up to a given height."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Labraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/william/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define a class for Actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Actor:\n",
    "    \n",
    "    def __init__(self, n_features, n_actions, learning_rate, discount):\n",
    "        self.n_features = n_features\n",
    "        self.n_actions = n_actions\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = discount\n",
    "        \n",
    "        self._build_model()\n",
    "        \n",
    "    def _build_model(self):\n",
    "        self.model = tf.keras.Sequential()\n",
    "        self.model.add(tf.keras.layers.Dense(16, activation='relu', input_shape=(self.n_features,)))\n",
    "        self.model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "        self.model.add(tf.keras.layers.Dense(self.n_actions, activation='softmax'))\n",
    "        \n",
    "        self.optimizer = tf.keras.optimizers.Adam(self.lr)\n",
    "    \n",
    "    def _grad(self, state, action, td_error):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss_value = self._loss(state, action, td_error)\n",
    "        return tape.gradient(loss_value, self.model.trainable_variables)\n",
    "        \n",
    "    def _loss(self, state, action, td_error):\n",
    "        probs = self.model(state)\n",
    "        \n",
    "        return - td_error * tf.math.log(probs[0, action])\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "        probs = self.model.predict(state)\n",
    "        action = np.random.choice(self.n_actions, p=probs.ravel())\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def learn(self, state, action, td_error):\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "        \n",
    "        grads = self._grad(state, action, td_error)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Define a class for Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Critic:\n",
    "    \n",
    "    def __init__(self, n_features, learning_rate, discount):\n",
    "        self.n_features = n_features\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = discount\n",
    "        \n",
    "        self._build_model()\n",
    "        \n",
    "    def _build_model(self):\n",
    "        self.model = tf.keras.Sequential()\n",
    "        self.model.add(tf.keras.layers.Dense(8, activation='relu', input_shape=(self.n_features,)))\n",
    "        self.model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "        self.model.add(tf.keras.layers.Dense(1, activation=None))\n",
    "        \n",
    "        self.optimizer = tf.keras.optimizers.Adam(self.lr)\n",
    "        \n",
    "    def _grad(self, state, next_state):\n",
    "        with tf.GradientTape() as tape:\n",
    "            td_error, loss_value = self._loss(state, next_state)\n",
    "        return td_error, tape.gradient(loss_value, self.model.trainable_variables)\n",
    "        \n",
    "    def _loss(self, state, next_state):\n",
    "        v = self.model(state)\n",
    "        v_ = self.model(next_state)\n",
    "        td_error = reward + self.gamma * v_ - v\n",
    "        \n",
    "        return td_error, tf.square(td_error)\n",
    "        \n",
    "    def learn(self, state, reward, next_state):\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "        next_state = np.expand_dims(next_state, axis=0)\n",
    "        \n",
    "        td_error, grads = self._grad(state, next_state)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "        \n",
    "        return td_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Let's train an A2C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1 Create an environment and wrap it by `Monitor` for recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/william/anaconda3/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.99926078,  0.03844342,  0.99999821, -0.00189067,  0.05553292,\n",
       "       -0.07179652])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('Acrobot-v1')\n",
    "env = Monitor(env, directory='./video',\n",
    "              video_callable=lambda count: count > 0 and count % 50 == 0)\n",
    "state = env.reset()\n",
    "state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2 Create an actor and a critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "actor = Actor(n_features=len(state), n_actions=env.action_space.n, learning_rate=0.001, discount=0.9)\n",
    "critic = Critic(n_features=len(state), learning_rate=0.001, discount=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3 Start to train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reward is designed that the value is 0 if the goal is reached, otherwise the value is -1.\n",
    "\n",
    "For simplicity, we don't need to modify the reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:   0, period: 499, reward: -500.000\n",
      "Episode:   1, period: 499, reward: -500.000\n",
      "Episode:   2, period: 499, reward: -500.000\n",
      "Episode:   3, period: 499, reward: -500.000\n",
      "Episode:   4, period: 321, reward: -321.000\n",
      "Episode:   5, period: 249, reward: -249.000\n",
      "Episode:   6, period:  85, reward: -85.000\n",
      "Episode:   7, period: 132, reward: -132.000\n",
      "Episode:   8, period:  97, reward: -97.000\n",
      "Episode:   9, period: 149, reward: -149.000\n",
      "Episode:  10, period:  93, reward: -93.000\n",
      "Episode:  11, period: 112, reward: -112.000\n",
      "Episode:  12, period:  95, reward: -95.000\n",
      "Episode:  13, period:  98, reward: -98.000\n",
      "Episode:  14, period: 109, reward: -109.000\n",
      "Episode:  15, period:  83, reward: -83.000\n",
      "Episode:  16, period:  99, reward: -99.000\n",
      "Episode:  17, period: 109, reward: -109.000\n",
      "Episode:  18, period:  84, reward: -84.000\n",
      "Episode:  19, period: 113, reward: -113.000\n",
      "Episode:  20, period:  97, reward: -97.000\n",
      "Episode:  21, period:  83, reward: -83.000\n",
      "Episode:  22, period: 200, reward: -200.000\n",
      "Episode:  23, period:  97, reward: -97.000\n",
      "Episode:  24, period:  81, reward: -81.000\n",
      "Episode:  25, period:  93, reward: -93.000\n",
      "Episode:  26, period:  85, reward: -85.000\n",
      "Episode:  27, period:  77, reward: -77.000\n",
      "Episode:  28, period:  75, reward: -75.000\n",
      "Episode:  29, period: 499, reward: -500.000\n",
      "Episode:  30, period:  83, reward: -83.000\n",
      "Episode:  31, period: 132, reward: -132.000\n",
      "Episode:  32, period:  82, reward: -82.000\n",
      "Episode:  33, period:  83, reward: -83.000\n",
      "Episode:  34, period: 104, reward: -104.000\n",
      "Episode:  35, period:  86, reward: -86.000\n",
      "Episode:  36, period:  99, reward: -99.000\n",
      "Episode:  37, period:  72, reward: -72.000\n",
      "Episode:  38, period: 102, reward: -102.000\n",
      "Episode:  39, period:  75, reward: -75.000\n",
      "Episode:  40, period:  86, reward: -86.000\n",
      "Episode:  41, period:  84, reward: -84.000\n",
      "Episode:  42, period: 199, reward: -199.000\n",
      "Episode:  43, period:  84, reward: -84.000\n",
      "Episode:  44, period: 135, reward: -135.000\n",
      "Episode:  45, period:  91, reward: -91.000\n",
      "Episode:  46, period:  77, reward: -77.000\n",
      "Episode:  47, period:  86, reward: -86.000\n",
      "Episode:  48, period:  90, reward: -90.000\n",
      "Episode:  49, period:  93, reward: -93.000\n",
      "Episode:  50, period: 137, reward: -137.000\n",
      "Episode:  51, period:  65, reward: -65.000\n",
      "Episode:  52, period: 136, reward: -136.000\n",
      "Episode:  53, period:  91, reward: -91.000\n",
      "Episode:  54, period:  87, reward: -87.000\n",
      "Episode:  55, period: 175, reward: -175.000\n",
      "Episode:  56, period:  78, reward: -78.000\n",
      "Episode:  57, period:  81, reward: -81.000\n",
      "Episode:  58, period:  97, reward: -97.000\n",
      "Episode:  59, period: 110, reward: -110.000\n",
      "Episode:  60, period:  64, reward: -64.000\n",
      "Episode:  61, period:  81, reward: -81.000\n",
      "Episode:  62, period: 121, reward: -121.000\n",
      "Episode:  63, period: 100, reward: -100.000\n",
      "Episode:  64, period: 102, reward: -102.000\n",
      "Episode:  65, period:  74, reward: -74.000\n",
      "Episode:  66, period:  74, reward: -74.000\n",
      "Episode:  67, period:  90, reward: -90.000\n",
      "Episode:  68, period:  83, reward: -83.000\n",
      "Episode:  69, period:  77, reward: -77.000\n",
      "Episode:  70, period: 101, reward: -101.000\n",
      "Episode:  71, period:  71, reward: -71.000\n",
      "Episode:  72, period:  78, reward: -78.000\n",
      "Episode:  73, period: 173, reward: -173.000\n",
      "Episode:  74, period:  63, reward: -63.000\n",
      "Episode:  75, period:  84, reward: -84.000\n",
      "Episode:  76, period: 115, reward: -115.000\n",
      "Episode:  77, period:  84, reward: -84.000\n",
      "Episode:  78, period: 105, reward: -105.000\n",
      "Episode:  79, period: 183, reward: -183.000\n",
      "Episode:  80, period:  79, reward: -79.000\n",
      "Episode:  81, period:  65, reward: -65.000\n",
      "Episode:  82, period:  72, reward: -72.000\n",
      "Episode:  83, period:  96, reward: -96.000\n",
      "Episode:  84, period:  81, reward: -81.000\n",
      "Episode:  85, period:  74, reward: -74.000\n",
      "Episode:  86, period:  65, reward: -65.000\n",
      "Episode:  87, period:  75, reward: -75.000\n",
      "Episode:  88, period: 103, reward: -103.000\n",
      "Episode:  89, period:  72, reward: -72.000\n",
      "Episode:  90, period:  91, reward: -91.000\n",
      "Episode:  91, period:  99, reward: -99.000\n",
      "Episode:  92, period:  75, reward: -75.000\n",
      "Episode:  93, period:  78, reward: -78.000\n",
      "Episode:  94, period:  97, reward: -97.000\n",
      "Episode:  95, period:  80, reward: -80.000\n",
      "Episode:  96, period:  83, reward: -83.000\n",
      "Episode:  97, period:  87, reward: -87.000\n",
      "Episode:  98, period:  93, reward: -93.000\n",
      "Episode:  99, period:  73, reward: -73.000\n",
      "game over\n"
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "periods = []\n",
    "rewards = []\n",
    "\n",
    "for episode in range(100):\n",
    "    period = 0\n",
    "    total_reward = 0\n",
    "    \n",
    "    state = env.reset()\n",
    "    \n",
    "    while True:\n",
    "        env.render()\n",
    "        \n",
    "        action = actor.choose_action(state)\n",
    "        \n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        total_reward += reward\n",
    "        \n",
    "        td_error = critic.learn(state, reward, next_state)\n",
    "        actor.learn(state, action, td_error)\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "        step += 1\n",
    "        period += 1\n",
    "    \n",
    "    print('Episode: {:3d}, period: {:3d}, reward: {:3.3f}'.format(episode, period, total_reward))\n",
    "    periods.append(period)\n",
    "    rewards.append(total_reward)\n",
    "\n",
    "env.close()\n",
    "print('game over')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow2",
   "language": "python",
   "name": "tensorflow2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
